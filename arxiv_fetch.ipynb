{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install feedparser scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#  arXiv   L L M / G P T 関連論文　投稿動向比較\n",
    "#          Period A : 2025-04-04 – 2025-05-03\n",
    "#          Period B : 2024-04-04 – 2024-05-03\n",
    "# ================================================================\n",
    "!pip -q install feedparser python-dateutil\n",
    "\n",
    "import math, sys, datetime as dt, itertools, requests, feedparser\n",
    "from urllib.parse import quote\n",
    "from dateutil import parser as dtp\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "# ---------- 1.  query parameters --------------------------------\n",
    "KEYWORDS = ['GPT', 'LLM', '\"Generative AI\"']        # 検索キーワード\n",
    "CATEGORIES = ['cs.CL', 'cs.AI']                     # arXiv カテゴリ\n",
    "MAX_PER_CALL = 2000                                 # API 1 回の最大取得件数\n",
    "\n",
    "PERIOD_A = (dt.date(2025, 4, 4), dt.date(2025, 5, 3))\n",
    "PERIOD_B = (dt.date(2024, 4, 4), dt.date(2024, 5, 3))\n",
    "\n",
    "# ---------- 2.  helper functions --------------------------------\n",
    "def build_query(start: dt.date, end: dt.date) -> str:\n",
    "    \"\"\"\n",
    "    arXiv API 用の search_query 文字列を URL エンコードして返す。\n",
    "    \"\"\"\n",
    "    kw_part  = ' OR '.join(f'all:{k}' for k in KEYWORDS)\n",
    "    cat_part = ' OR '.join(f'cat:{c}' for c in CATEGORIES)\n",
    "\n",
    "    # end_date は上限を含めたいので翌日 00:00:00 まで開区間\n",
    "    date_part = (f'submittedDate:[{start.strftime(\"%Y%m%d%H%M%S\")} TO '\n",
    "                 f'{(end + dt.timedelta(days=1)).strftime(\"%Y%m%d%H%M%S\")}]')\n",
    "\n",
    "    raw = f'({kw_part}) AND ({cat_part}) AND {date_part}'\n",
    "    return quote(raw, safe='')       # 空白も含めて完全エンコード\n",
    "\n",
    "def harvest(start: dt.date, end: dt.date):\n",
    "    \"\"\"\n",
    "    指定期間の論文をすべて harvest し、published 日付の list を返す。\n",
    "    \"\"\"\n",
    "    q = build_query(start, end)\n",
    "    idx, dates = 0, []\n",
    "\n",
    "    print(f'  fetching {start} – {end} … ', end=''); sys.stdout.flush()\n",
    "    while True:\n",
    "        url = (f'https://export.arxiv.org/api/query?'\n",
    "               f'search_query={q}&start={idx}&max_results={MAX_PER_CALL}&'\n",
    "               f'sortBy=submittedDate&sortOrder=ascending')\n",
    "        feed = feedparser.parse(url)\n",
    "        if not feed.entries:\n",
    "            break\n",
    "        dates += [dtp.parse(e.published).date() for e in feed.entries]\n",
    "        idx   += MAX_PER_CALL\n",
    "    print(f'done  ({len(dates)} entries)')\n",
    "    return dates\n",
    "\n",
    "def to_daily_series(date_list, start: dt.date, end: dt.date) -> pd.Series:\n",
    "    \"\"\"\n",
    "    日毎カウント (pd.Series) へ変換。欠落日は 0 で埋める。\n",
    "    \"\"\"\n",
    "    s = (pd.Series(1, index=pd.to_datetime(date_list))\n",
    "           .resample('D').sum())\n",
    "    idx = pd.date_range(start, end, freq='D')\n",
    "    return s.reindex(idx, fill_value=0)\n",
    "\n",
    "# ---------- 3.  data download & aggregation ---------------------\n",
    "dates_A = harvest(*PERIOD_A)\n",
    "dates_B = harvest(*PERIOD_B)\n",
    "\n",
    "ser_A = to_daily_series(dates_A, *PERIOD_A)\n",
    "ser_B = to_daily_series(dates_B, *PERIOD_B)\n",
    "\n",
    "n_days = len(ser_A)                                   # 30 日\n",
    "\n",
    "S_A, S_B = ser_A.sum(), ser_B.sum()\n",
    "lam_A, lam_B = S_A / n_days, S_B / n_days\n",
    "\n",
    "print('\\n=== descriptive statistics ===')\n",
    "print(f'S_A = {S_A:4d}  ⇒  λ̂_A = {lam_A:6.2f} /day')\n",
    "print(f'S_B = {S_B:4d}  ⇒  λ̂_B = {lam_B:6.2f} /day')\n",
    "\n",
    "# ---------- 4.  likelihood-ratio test (Poisson rates) -----------\n",
    "lam_pool = (S_A + S_B) / (2 * n_days)                 # 帰無仮説下の共通率\n",
    "\n",
    "def safe_term(S):\n",
    "    \"\"\"0 log 0 問題を回避\"\"\"\n",
    "    return 0.0 if S == 0 else S * math.log(S / (n_days * lam_pool))\n",
    "\n",
    "G = 2.0 * (safe_term(S_A) + safe_term(S_B))\n",
    "p_val = stats.chi2.sf(G, df=1)\n",
    "\n",
    "print('\\n=== LRT  Poisson rates ===')\n",
    "print(f'G = {G:8.3f},   p-value = {p_val:.3e}')\n",
    "print('→', '差が **有意あり**' if p_val < 0.05 else '差は **有意ではない**')\n",
    "\n",
    "# ---------- 5.  quick visualisation ------------------------------\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(ser_B.index, ser_B, label='2024-04-04 – 05-03', lw=2)\n",
    "plt.plot(ser_A.index, ser_A, label='2025-04-04 – 05-03', lw=2)\n",
    "plt.ylabel('daily submissions')\n",
    "plt.grid(alpha=.3); plt.legend()\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
